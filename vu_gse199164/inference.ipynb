{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3afbcbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from interpret.glassbox import ExplainableBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56a00226",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = Path(\"../src/models\")\n",
    "\n",
    "with open(models_path / \"CORR_flync_ebm_model_redux.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed2d1349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 54)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_data_path = Path(\"../vu_gse199164/vu_gse199164.parquet\")\n",
    "\n",
    "df = pd.read_parquet(inf_data_path)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df[df.duplicated(subset=[\"chromosome\", \"start\", \"end\"])]\n",
    "duplicates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9776f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(\"index\", inplace=True)\n",
    "\n",
    "cols_to_drop = ['chromosome', 'start', 'end', 'ss_sequence', 'ss_structure'] + [col for col in df.columns if col.startswith('cpat')]\n",
    "cols_to_drop\n",
    "\n",
    "df.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "469a4e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling NAs in column cov_epdnew with 0\n",
      "Filling NAs in column cov_h3k4me3 with 0\n",
      "Filling NAs in column cov_s2_pol2 with 0\n",
      "Filling NAs in column cov_tss_minus with 0\n",
      "Filling NAs in column cov_tss_plus with 0\n",
      "Filling NAs in column max_s2_pol2 with 0\n",
      "Filling NAs in column max_tss_plus with 0\n",
      "Filling NAs in column mean_h3k4me3 with 0\n",
      "Filling NAs in column mean_pcons27 with 0\n",
      "Filling NAs in column mean_phylocons124 with 0\n",
      "Filling NAs in column min_tss_minus with 0\n",
      "Filling NAs in column std__phylocons124 with 0\n",
      "Filling NAs in column std_pcons27 with 0\n",
      "Filling NAs in column sum__phylocons124 with 0\n",
      "Filling NAs in column sum_h3k4me3 with 0\n",
      "Filling NAs in column sum_pcons27 with 0\n",
      "Filling NAs in column sum_s2_pol2 with 0\n",
      "Filling NAs in column sum_tss_minus with 0\n",
      "Filling NAs in column sum_tss_plus with 0\n",
      "Rows with ss_mfe >= 0 dropped. Shape: (18956, 38)\n",
      "Filling NAs in column entries_remap with empty string\n"
     ]
    }
   ],
   "source": [
    "# Logic for data cleaning:\n",
    "# 1. If columns are UCSC (BigWig or BigBed) statistics features or CPAT scores (no ORFs found), fill NAs with 0\n",
    "logic_op1 = (df.columns.str.startswith(tuple(['min_', 'max_', 'mean_', 'std_', 'sum_', 'cov_'])) | df.columns.str.startswith('cpat_'))\n",
    "for col in df.columns:\n",
    "    if col in df.columns[logic_op1]:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            print(f\"Filling NAs in column {col} with 0\")\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "# 2. If columns are ss_* features, drop rows with NAs. This seems to be the best approach as no structure was calculated for the sequence\n",
    "logic_op2 = df.columns.str.startswith('ss_')\n",
    "for col in df.columns:\n",
    "    if col in df.columns[logic_op2]:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            print(f\"Dropping rows with NAs in column {col}\")\n",
    "            df = df.dropna(subset=[col])\n",
    "            print(f\"Rows with NAs in column {col} dropped. Shape: {df.shape}\")\n",
    "        # Drop values where `ss_mfe` is not < 0\n",
    "        if col == 'ss_mfe':\n",
    "            df = df[df[col] < 0]\n",
    "            print(f\"Rows with ss_mfe >= 0 dropped. Shape: {df.shape}\")\n",
    "\n",
    "# 3. If columns start with '0' or '1', drop rows with NAs. This seems to be the best approach as we have no counts for all required k-mers\n",
    "logic_op3 = df.columns.str.startswith(('0', '1')) | df.columns.str.contains('mer_SVD')\n",
    "for col in df.columns:\n",
    "    if col in df.columns[logic_op3]:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            print(f\"Dropping rows with NAs in column {col}\")\n",
    "            df = df.dropna(subset=[col])\n",
    "            print(f\"Rows with NAs in column {col} dropped. Shape: {df.shape}\")\n",
    "\n",
    "# 4. If columns are categorical, fill NAs with ''\n",
    "logic_op4 = df.dtypes == 'object'\n",
    "for col in df.columns:\n",
    "    if col in df.columns[logic_op4]:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            print(f\"Filling NAs in column {col} with empty string\")\n",
    "            df[col] = df[col].fillna('')\n",
    "\n",
    "    # remove trailing '_[1-9]' digits from entries_epdnew column. These are promoter rankings which are not needed.\n",
    "    if 'entries_epdnew' in df.columns:\n",
    "        df['entries_epdnew'] = df['entries_epdnew'].str.split(',')\n",
    "        # Remove '_[any digit]' suffix from each entry, even if two or more digits\n",
    "        df[\"entries_epdnew\"] = df[df['entries_epdnew'].notna()]['entries_epdnew'].apply(lambda x: ','.join([re.sub(r'_[0-9]+$', '', i) for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b9aac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-hot encoding\n",
    "# Get a df with only the categorical features.\n",
    "df_categorical = df.loc[:, df.dtypes == 'object']\n",
    "\n",
    "mhe_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "prefix_cols = True\n",
    "\n",
    "# For each categorical feature, perform multi-hot encoding\n",
    "for col in df_categorical.columns:\n",
    "    prefix = col.split('_')[-1]\n",
    "\n",
    "    vectorizer = CountVectorizer(tokenizer=lambda x: x.split(','), binary=True, token_pattern=None)\n",
    "\n",
    "    # Only fit on non-empty values to avoid creating a column for ''\n",
    "    non_empty = df_categorical[col][df_categorical[col] != '']\n",
    "    if not non_empty.empty:\n",
    "        vectorizer.fit(non_empty)\n",
    "        # Transform all values (including empty) to ensure shape matches\n",
    "        encoded = vectorizer.transform(df_categorical[col])\n",
    "        encoded_df = pd.DataFrame(encoded.toarray(), columns=vectorizer.get_feature_names_out(), index=df.index)\n",
    "        # Convert to bool dtype\n",
    "        encoded_df = encoded_df.astype(bool)\n",
    "        if prefix_cols:\n",
    "            encoded_df.columns = [f\"{prefix}_{c}\" for c in encoded_df.columns]\n",
    "        mhe_df = pd.concat([mhe_df, encoded_df], axis=1)\n",
    "    else:\n",
    "        # If all values are empty, skip this column\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65863674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18956, 695)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mhe_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d737b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if duplicated columns exist in the multi-hot encoded DataFrame\n",
    "if mhe_df.columns.duplicated().any():\n",
    "    print(\"Duplicated columns found in the multi-hot encoded DataFrame. Removing duplicates.\")\n",
    "    # Merge duplicated columns by taking the logical OR (future-proof way)\n",
    "    mhe_df = mhe_df.T.groupby(level=0).any().T.astype(bool)\n",
    "\n",
    "# Check if duplicated entries from different BigBed files.\n",
    "# For each column <prefix>_<col_name> if <col_name> matches, merge them to a single one\n",
    "# and change the column name to merged_<col_name> using the logical OR approach to keeping 1s\n",
    "if prefix_cols:\n",
    "    merged_columns = {}\n",
    "    for col in mhe_df.columns:\n",
    "        prefix, col_name = col.split('_', 1)\n",
    "        if col_name not in merged_columns:\n",
    "            merged_columns[col_name] = []\n",
    "        merged_columns[col_name].append(col)\n",
    "\n",
    "    for col_name, cols in merged_columns.items():\n",
    "        if len(cols) > 1:\n",
    "            # Check if all values are identical across the columns for this col_name\n",
    "            all_equal = mhe_df[cols].nunique(axis=1).eq(1).all()\n",
    "\n",
    "            if all_equal:\n",
    "                # Safe to merge because all rows have the same value\n",
    "                mhe_df[f'merged_{col_name}'] = mhe_df[cols].iloc[:, 0]\n",
    "                mhe_df = mhe_df.drop(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "922d884c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18956, 695)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mhe_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e79f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the multi-hot encoded features with the original DataFrame. Making sure the indexes match\n",
    "df = pd.concat([df.drop(columns=df_categorical.columns), mhe_df], axis=1)\n",
    "\n",
    "# Check columns for invalid characters and rename them\n",
    "df.columns = df.columns.str.replace(r'[^a-zA-Z0-9_]', '_', regex=True)\n",
    "df.columns = df.columns.str.replace(r'__+', '_', regex=True)\n",
    "df.columns = df.columns.str.strip('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d7a59ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['10mer_svd1', '11mer_svd1', '12mer_svd1', '3mer_svd1', '4mer_svd1',\n",
       "       '5mer_svd1', '6mer_svd1', '7mer_svd1', '8mer_svd1', '9mer_svd1',\n",
       "       ...\n",
       "       'tfbs_usp', 'tfbs_vfl', 'tfbs_vis', 'tfbs_vnd', 'tfbs_vsx1',\n",
       "       'tfbs_vsx2', 'tfbs_vvl', 'tfbs_z', 'tfbs_zen', 'tfbs_zen2'],\n",
       "      dtype='object', length=731)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort columns alphabetically\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "\n",
    "# Columns to lowercase\n",
    "df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36b331e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(df)\n",
    "y_probab = model.predict_proba(df)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a65e0bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach predictions to the original DataFrame\n",
    "df['prediction'] = y_pred\n",
    "df['probability'] = y_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8a1222eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df.drop(columns=[col for col in df.columns if col not in ['prediction', 'probability']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d732032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(inf_data_path)\n",
    "df = df[['index', 'chromosome', 'start', 'end']]\n",
    "df.set_index('index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "03ac8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df.merge(res, how='inner', right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "af804527",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_excel(\"flync_vu_gse199164_inference_results.xlsx\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flync",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
