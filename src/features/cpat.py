import logging
import os
import sys
import tempfile
import shutil
import pandas as pd
import pyranges as pr
import numpy as np # Added for np.nan
import argparse  # Added for command line argument parsing

# Assuming cpmodule is installed and accessible in the environment
try:
    from cpmodule import FrameKmer, fickett, find_orfs
    from cpmodule.utils import bed_or_fasta
    from cpmodule.utils import coding_prediction, index_fasta, seq_from_bed
except ImportError as e:
    print(f"Error importing cpmodule: {e}", file=sys.stderr)
    print("Please ensure cpmodule is installed and accessible.", file=sys.stderr)
    sys.exit(1)

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s"
)

# --- Constants ---
# Standard BED12 columns often expected by tools like seq_from_bed
BED12_COLS = [
    "Chromosome", "Start", "End", "Name", "Score", "Strand",
    "ThickStart", "ThickEnd", "ItemRGB", "BlockCount", "BlockSizes", "BlockStarts"
]
# Columns generated by this script before CPAT R call
ORF_INFO_HEADER = [
    "ORF_ID", "Transcript_ID", "Transcript_Len", "ORF_Strand", "ORF_Frame",
    "ORF_Start_Transcript", "ORF_End_Transcript", "ORF_Len",
    "Fickett_Score", "Hexamer_Score"
]
# Columns expected in the CPAT R script output (adjust if different)
CPAT_OUTPUT_HEADER = [
    "ORF_ID", "Transcript_Len", "ORF_Len", "Fickett_Score",
    "Hexamer_Score", "Coding_prob"
]


def check_dependencies(rscript_check: bool = True):
    """Check for essential external dependencies."""
    logging.info("Checking dependencies...")
    if rscript_check:
        if not shutil.which("Rscript"):
            logging.error("-----------------------------------------")
            logging.error("Dependency Error: 'Rscript' command not found.")
            logging.error("Please install R and ensure 'Rscript' is in your system PATH.")
            logging.error("See: https://cran.r-project.org/")
            logging.error("-----------------------------------------")
            return False
        else:
            logging.info("- Rscript found.")
    # Can add checks for other command-line tools if needed
    logging.info("Dependency check passed.")
    return True


def load_hexamer_scores(hexamer_dat_path):
    """Loads hexamer scores from the provided file."""
    if not os.path.exists(hexamer_dat_path):
        logging.error(f"Hexamer file not found: {hexamer_dat_path}")
        raise FileNotFoundError(f"Hexamer file not found: {hexamer_dat_path}")

    coding = {}
    noncoding = {}
    logging.info(f"Reading hexamer scores from {hexamer_dat_path}")
    try:
        with open(hexamer_dat_path, "r") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("hexamer"):
                    continue
                fields = line.split()
                if len(fields) == 3:
                    coding[fields[0]] = float(fields[1])
                    noncoding[fields[0]] = float(fields[2])
                else:
                    logging.warning(f"Skipping malformed line in hexamer file: {line}")
    except Exception as e:
        logging.error(f"Error reading hexamer file {hexamer_dat_path}: {e}")
        raise
    return coding, noncoding


def run_cpat_calculation(
    bed_file_path: str,
    ref_genome_path: str,
    hexamer_dat_path: str,
    logit_model_path: str,
    output_parquet_path: str,
    min_orf_len: int = 75,
    n_top_orf: int = 5,  # Keep multiple ORFs initially
    start_codons: list = ["ATG"],
    stop_codons: list = ["TAG", "TAA", "TGA"],
    antisense: bool = False,
) -> pd.DataFrame:
    """
    Runs CPAT calculations on a BED file using cpmodule functions,
    aggregating results to provide the best score per transcript.
    Ensures all input transcripts are present in the output, with NA for missing data.

    Args:
        bed_file_path: Path to the input BED file (BED6 or BED12 recommended).
        ref_genome_path: Path to the reference genome FASTA file.
        hexamer_dat_path: Path to the hexamer frequency table file.
        logit_model_path: Path to the logistic regression model (.RData file).
        output_parquet_path: Path to save the final aggregated results per transcript.
        min_orf_len: Minimum ORF length in nucleotides.
        n_top_orf: Max number of ORF candidates *per strand* to consider per transcript.
        start_codons: List of start codons.
        stop_codons: List of stop codons.
        antisense: Whether to search ORFs on the antisense strand.

    Returns:
        A pandas DataFrame containing the CPAT result for all input transcripts.
    """
    # Define final columns early for use in various scenarios
    final_columns = [
        'Transcript_ID', 'Coding_prob', 'ORF_ID', 'ORF_Len', 'Fickett_Score',
        'Hexamer_Score', 'ORF_Strand', 'ORF_Frame', 'ORF_Start_Transcript',
        'ORF_End_Transcript', 'Transcript_Len'
    ]

    logging.info("Starting CPAT calculation...")

    # 0. Pre-flight Checks
    required_files = {
        "BED": bed_file_path,
        "Reference FASTA": ref_genome_path,
        "Hexamer Table": hexamer_dat_path,
        "Logit Model": logit_model_path,
    }
    for name, path in required_files.items():
        if not os.path.exists(path):
            logging.error(f"Input Error: {name} file not found at {path}")
            raise FileNotFoundError(f"{name} file not found: {path}")

    if not check_dependencies(rscript_check=True):
        raise EnvironmentError("Missing required external dependencies (Rscript).")

    # 1. Load Hexamer Scores
    coding_hex, noncoding_hex = load_hexamer_scores(hexamer_dat_path)

    # 2. Index Reference Genome (if needed by seq_from_bed)
    # Consider if indexing is always needed or if seq_from_bed handles it.
    # Assuming it's cheap enough to do preventatively.
    logging.info(f"Ensuring reference genome is indexed: {ref_genome_path}")
    try:
        # index_fasta usually creates a .fai file
        if not os.path.exists(ref_genome_path + ".fai"):
             index_fasta(ref_genome_path)
        else:
             logging.info("FASTA index (.fai) already exists.")
    except Exception as e:
        logging.warning(f"Failed to index reference genome (may cause issues if seq_from_bed requires it): {e}")
        # Decide if this should be a fatal error
        # raise e # Uncomment to make indexing failure fatal

    # 3. Read BED file with pyranges
    logging.info(f"Reading BED file: {bed_file_path}")
    try:
        # Assume input might be BED6, try to provide defaults for BED12 columns
        # if seq_from_bed requires them. The most crucial are Name and Strand.
        pr_bed = pr.read_bed(bed_file_path)
        df_bed = pr_bed.df

        # Ensure Name and Strand columns exist
        if "Name" not in df_bed.columns:
             if "name" in df_bed.columns: # Check for lowercase 'name'
                df_bed = df_bed.rename(columns={"name":"Name"})
             else:
                logging.warning("BED file lacks 'Name' column (column 4). Using index as transcript ID.")
                # Create a unique ID based on chrom/start/end/strand if possible
                df_bed["Name"] = df_bed.apply(lambda r: f"{r.Chromosome}:{r.Start}-{r.End}({r.get('Strand','+')})_idx{r.name}", axis=1)

        if "Strand" not in df_bed.columns:
            logging.warning("BED file lacks 'Strand' column (column 6). Assuming '+' strand.")
            df_bed["Strand"] = "+" # Default to positive strand

        # Prepare for seq_from_bed which expects a BED string
        # Add defaults for potentially missing BED12 columns needed for string reconstruction
        # Default 'Score' to 0
        if 'Score' not in df_bed.columns: df_bed['Score'] = 0
        # Default ThickStart/End to Start/End
        if 'ThickStart' not in df_bed.columns: df_bed['ThickStart'] = df_bed['Start']
        if 'ThickEnd' not in df_bed.columns: df_bed['ThickEnd'] = df_bed['End']
        # Default ItemRGB to black
        if 'ItemRGB' not in df_bed.columns: df_bed['ItemRGB'] = '0,0,0'
        # Calculate BlockCount, BlockSizes, BlockStarts if missing (assuming single exon if no info)
        if 'BlockCount' not in df_bed.columns: df_bed['BlockCount'] = 1
        if 'BlockSizes' not in df_bed.columns: df_bed['BlockSizes'] = df_bed['End'] - df_bed['Start']
        if 'BlockStarts' not in df_bed.columns: df_bed['BlockStarts'] = 0

        # Ensure columns are in the correct order for BED12 string format
        # Select existing columns and add missing ones with defaults if needed
        # (The reconstruction below handles ordering explicitly)

    except Exception as e:
        logging.error(f"Failed to read or process BED file with pyranges: {e}")
        raise

    # Store all unique transcript IDs from the input BED
    all_transcript_ids_master_df = df_bed[['Name']].rename(columns={'Name': 'Transcript_ID'}).drop_duplicates().reset_index(drop=True)

    # 4. Process BED entries and find ORFs
    orf_info_data = []  # To store data for the intermediate TSV
    rna_lengths_data = [] # To store (transcript_id, rna_length)
    processed_count = 0
    error_count = 0
    no_orf_entries = [] # Transcripts where no ORFs were found in their sequence

    logging.info("Processing BED entries and searching for ORFs...")
    # Reconstruct BED lines from the DataFrame
    bed_lines = []
    transcript_ids = [] # Keep track of original transcript IDs
    for _, row in df_bed.iterrows():
        # Construct a BED12 formatted string line, ensuring correct order and types
        # Use .get() for columns that might *still* be missing despite defaults
        try:
            line = "\t".join(map(str, [
                row["Chromosome"], row["Start"], row["End"], row["Name"],
                row.get("Score", 0), row.get("Strand", "+"),
                row.get("ThickStart", row["Start"]), row.get("ThickEnd", row["End"]),
                row.get("ItemRGB", "0,0,0"), row.get("BlockCount", 1),
                row.get("BlockSizes", row["End"] - row["Start"]),
                row.get("BlockStarts", 0)
            ]))
            bed_lines.append(line)
            transcript_ids.append(row["Name"])
        except KeyError as e:
             logging.error(f"Missing expected BED column for reconstruction: {e} in row: {row.to_dict()}")
             error_count += 1
        except Exception as e:
             logging.error(f"Error reconstructing BED line for row {row.get('Name', 'Unknown')}: {e}")
             error_count += 1


    for i, bed_line in enumerate(bed_lines):
        processed_count += 1
        if processed_count % 500 == 0:
            logging.info(f"Processed {processed_count}/{len(bed_lines)} BED entries...")

        transcript_id = transcript_ids[i] # Get the corresponding transcript ID

        try:
            # seq_from_bed extracts sequence based on BED blocks
            name_from_seq, seq = seq_from_bed(bed_line, ref_genome_path)
            
            if not seq:
                logging.warning(f"Could not extract sequence for {transcript_id}. Skipping ORF search for this transcript.")
                # rna_lengths_data will not have an entry, leading to NaN for Transcript_Len later, which is correct.
                # Consider if this should be counted in error_count or a separate counter for seq extraction failures.
                # For now, it won't be in orf_info_data.
                continue # Skip ORF finding for this transcript

            RNA_len = len(seq)
            # Successfully extracted sequence, record its length
            rna_lengths_data.append({'Transcript_ID': transcript_id, 'Transcript_Len': RNA_len})
            
            # find_orfs expects uppercase sequence
            orf_finder = find_orfs.ORFFinder(seq=seq.upper(), min_orf=min_orf_len)
            ORFs = orf_finder.orf_candidates(
                antisense=antisense,
                n_candidate=n_top_orf, # Max candidates *per strand*
                start_coden=start_codons,
                stop_coden=stop_codons,
            )

            if not ORFs:
                # Log only if verbose or if it's unexpected
                # logging.debug(f"No ORFs found meeting criteria for {transcript_id}")
                no_orf_entries.append(transcript_id)
                continue

            orf_sn = 1  # ORF serial number per transcript
            for orf in ORFs:
                # orf format: (direction, frame_number+1, orf_start_0based, orf_end_0based, length, sequence)
                orf_seq = orf[-1]
                orf_len = orf[-2]
                orf_strand = orf[0] # '+' or '-' relative to extracted RNA sequence
                orf_frame = orf[1]
                # Coordinates relative to the *start* of the extracted sequence
                orf_start_rel = orf[2] + 1
                orf_end_rel = orf[3] # End coord is inclusive in this context? Check cpmodule doc. Usually python is exclusive. Assume 0-based exclusive for end.
                # Let's assume cpmodule provides 0-based start, 0-based exclusive end coordinate relative to seq
                orf_start_rel_0based = orf[2]
                orf_end_rel_0based = orf[3]
                # Verify length calculation
                if len(orf_seq) != orf_len:
                     logging.warning(f"ORF length mismatch for {transcript_id} ORF {orf_sn}. Reported: {orf_len}, Sequence: {len(orf_seq)}")
                if orf_len != (orf_end_rel_0based - orf_start_rel_0based):
                     logging.warning(f"ORF coordinate mismatch for {transcript_id} ORF {orf_sn}. Len: {orf_len}, Coords: {orf_start_rel_0based}-{orf_end_rel_0based}")

                # Generate unique ORF ID: TranscriptID_ORF_SerialNumber
                orf_id = f"{transcript_id}_ORF_{orf_sn}"

                # Calculate scores
                fickett_score = fickett.fickett_value(orf_seq)
                # FrameKmer expects sequence, k-mer size, step size, coding dict, noncoding dict
                hexamer_score = FrameKmer.kmer_ratio(orf_seq, 6, 3, coding_hex, noncoding_hex)

                # Store info needed for coding_prediction TSV input
                # Adjust header based on what coding_prediction R script actually needs.
                # Assuming it needs: ID, mRNA_len, ORF_len, Fickett, Hexamer
                # Let's provide more context in our intermediate data.
                orf_info_data.append([
                    orf_id,
                    transcript_id,
                    RNA_len,
                    orf_strand,
                    orf_frame,
                    orf_start_rel_0based + 1, # Convert to 1-based start for output convention
                    orf_end_rel_0based,      # Keep 0-based exclusive end? Or convert to 1-based inclusive? Let's use 1-based inclusive.
                    orf_len,
                    fickett_score,
                    hexamer_score,
                ])
                orf_sn += 1

        except find_orfs.ORFSearchError as e:
             logging.warning(f"ORF finding error for {transcript_id}: {e}")
             error_count += 1
        except Exception as e:
            logging.error(
                f"Error processing entry {transcript_id} (BED line starts '{bed_line[:50]}...'): {e}",
                exc_info=False, # Set to True for full traceback if debugging
            )
            error_count += 1

    logging.info(f"Finished ORF finding.")
    logging.info(f"- Found ORFs for {len(orf_info_data)} candidates across transcripts.")
    logging.info(f"- Entries with no ORFs found or seq errors: {len(no_orf_entries) + error_count}")
    logging.info(f"- Other processing errors: {error_count}")

    # Create DataFrame of all transcript IDs with their lengths
    df_transcript_lengths = pd.DataFrame(rna_lengths_data)
    if df_transcript_lengths.empty:
        all_transcripts_with_len_df = all_transcript_ids_master_df.copy()
        all_transcripts_with_len_df['Transcript_Len'] = np.nan
    else:
        # Ensure unique transcript_id entries in df_transcript_lengths before merge
        df_transcript_lengths = df_transcript_lengths.drop_duplicates(subset=['Transcript_ID'])
        all_transcripts_with_len_df = pd.merge(all_transcript_ids_master_df, df_transcript_lengths, on='Transcript_ID', how='left')


    if not orf_info_data:
        logging.info("No valid ORFs were identified from any transcript sequences. "
                     "The output will contain NA for ORF-specific fields for all transcripts.")
        final_df = all_transcripts_with_len_df.copy()
        for col_name in final_columns:
            if col_name not in final_df.columns:
                final_df[col_name] = np.nan
        final_df = final_df.reindex(columns=final_columns, fill_value=np.nan) # Ensure order and all columns

        logging.info(f"Saving final aggregated results per transcript to {output_parquet_path}")
        try:
            os.makedirs(os.path.dirname(output_parquet_path), exist_ok=True)
            final_df.to_parquet(output_parquet_path, index=False)
        except Exception as e_save:
            logging.error(f"Failed to save results to Parquet file (no ORFs found scenario): {e_save}")
            raise # Reraise saving error
        logging.info("CPAT calculation (no ORFs found path) finished successfully.")
        logging.info(f"Output saved to: {output_parquet_path}")
        return final_df

    # 5. Write intermediate ORF info file for coding_prediction R script
    # Use a temporary directory for intermediate files
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_output_prefix = os.path.join(temp_dir, "cpat_temp_output")
        # Input file for R script
        orf_info_tsv_path = f"{temp_output_prefix}.ORF_info_input.tsv"
        # Expected output file from R script
        orf_prob_tsv_path = f"{temp_output_prefix}.ORF_prob.tsv"

        logging.info(f"Writing intermediate ORF info for R script to {orf_info_tsv_path}")
        try:
            # Create DataFrame with the specific columns needed by coding_prediction's R script
            # Typically: ID, mRNA, ORF, Fickett, Hexamer
            # Map our detailed columns to these expected names
            df_orf_input = pd.DataFrame(orf_info_data, columns=ORF_INFO_HEADER)
            df_r_input = df_orf_input.rename(columns={
                 "ORF_ID": "ID",
                 "Transcript_Len": "mRNA",
                 "ORF_Len": "ORF",
                 "Fickett_Score": "Fickett",
                 "Hexamer_Score": "Hexamer"
            })
            # Select only the columns expected by the R script's input
            # Make sure this matches what coding_prediction expects!
            r_input_cols = ["ID", "mRNA", "ORF", "Fickett", "Hexamer"]
            if not all(col in df_r_input.columns for col in r_input_cols):
                 missing_cols = [c for c in r_input_cols if c not in df_r_input.columns]
                 raise ValueError(f"Internal error: Missing columns required for R input: {missing_cols}")

            df_r_input[r_input_cols].to_csv(
                orf_info_tsv_path, sep="\t", index=False, header=True
            )
            # Keep our more detailed DataFrame for later merging
            df_orf_details = df_orf_input.set_index("ORF_ID")

        except Exception as e:
            logging.error(f"Failed to write intermediate ORF info TSV for R script: {e}")
            raise

        # 6. Run coding_prediction (which calls the R script)
        logging.info("Calculating coding probability using the logit model via Rscript...")
        
        results_df_raw = pd.DataFrame() # Initialize empty DataFrame for R results
        cpat_scores_successfully_loaded = False

        try:
            # This function executes the R script using the RData model.
            # It reads orf_info_tsv_path and writes orf_prob_tsv_path.
            coding_prediction(logit_model_path, orf_info_tsv_path, temp_output_prefix)
            
            if not os.path.exists(orf_prob_tsv_path):
                 raise FileNotFoundError(f"R script completed but did not create the expected output file: {orf_prob_tsv_path}")
            if os.path.getsize(orf_prob_tsv_path) == 0:
                 raise ValueError(f"R script created an empty output file: {orf_prob_tsv_path}")

            logging.info(f"Reading coding probability results from {orf_prob_tsv_path}")
            results_df_raw = pd.read_csv(orf_prob_tsv_path, sep="\t")
            
            if "ID" not in results_df_raw.columns or "Coding_prob" not in results_df_raw.columns:
                 logging.error(f"CPAT output TSV {orf_prob_tsv_path} is missing expected columns ('ID', 'Coding_prob'). Found: {results_df_raw.columns.tolist()}")
                 # cpat_scores_successfully_loaded remains False
            else:
                results_df_raw = results_df_raw.rename(columns={"ID": "ORF_ID"})
                results_df_raw = results_df_raw.set_index("ORF_ID")
                if 'Coding_prob' in results_df_raw.columns: # Double check
                    cpat_scores_successfully_loaded = True
                    logging.info("R script execution and result loading seem successful.")
                else: # Should be caught by the column check above
                    logging.error(f"CPAT output TSV {orf_prob_tsv_path} read but 'Coding_prob' column is missing.")

        except FileNotFoundError as e:
            logging.error(f"Error related to R script output file: {e}")
            logging.error("This likely means the R script failed or did not produce the expected output.")
            logging.error("Check R installation, PATH, and potential R errors if logged elsewhere.")
            # cpat_scores_successfully_loaded remains False
        except ValueError as e: # For empty file or issues with content like missing columns
            logging.error(f"Error processing R script output file ({orf_prob_tsv_path}): {e}")
            # cpat_scores_successfully_loaded remains False
        except Exception as e:
            logging.error(f"General error during coding_prediction (R script execution or output processing): {e}", exc_info=True)
            # cpat_scores_successfully_loaded remains False
            if not os.path.exists(orf_prob_tsv_path):
                logging.error(f"Expected R output file {orf_prob_tsv_path} was not created or accessible after error.")
        
        # 7. Read the R script's results - This section is now integrated above.

        # 8. Combine R results with ORF details
        logging.info("Combining ORF details with coding probabilities (if available)...")
        if not cpat_scores_successfully_loaded or results_df_raw.empty:
            logging.warning("CPAT scores ('Coding_prob') are not available or failed to load. 'Coding_prob' will be NA for all ORFs.")
            combined_df = df_orf_details.copy() # df_orf_details is indexed by ORF_ID
            combined_df['Coding_prob'] = np.nan
        else:
            # Join ORF details with R results. Use left join to keep all ORFs from df_orf_details,
            # adding Coding_prob if available from results_df_raw.
            combined_df = df_orf_details.join(results_df_raw[['Coding_prob']], how='left')
            # ORFs in df_orf_details but not in results_df_raw (e.g. R script filtered them) will have NaN for Coding_prob.
            if combined_df['Coding_prob'].isnull().any():
                 logging.warning("Some ORFs have missing 'Coding_prob' after join. This may be due to R script filtering or partial results.")
        
        # Ensure 'Transcript_ID' and 'ORF_ID' are columns for sorting and grouping
        if combined_df.index.name == 'ORF_ID':
            combined_df = combined_df.reset_index() # ORF_ID becomes a column
        
        # 9. Aggregate results per transcript: Find the ORF with the highest coding probability
        logging.info("Aggregating results: selecting best ORF per transcript...")
        
        best_orfs_found_df = pd.DataFrame() # Initialize empty

        if 'Transcript_ID' not in combined_df.columns and not combined_df.empty:
             logging.error("Internal Error: 'Transcript_ID' column is missing in combined_df before aggregation.")
             # Create an empty df with expected columns to allow merge to proceed and fill with NAs
             best_orfs_found_df = pd.DataFrame(columns=['Transcript_ID'] + [col for col in final_columns if col != 'Transcript_ID'])
        elif combined_df.empty:
             logging.info("combined_df is empty (e.g., no ORFs passed to R or R data issue). No ORFs to aggregate.")
             best_orfs_found_df = pd.DataFrame(columns=['Transcript_ID'] + [col for col in final_columns if col != 'Transcript_ID'])
        else:
            # Sort by Transcript ID and then by Coding Probability (descending)
            # NaNs in 'Coding_prob' will be sorted last by default.
            combined_df_sorted = combined_df.sort_values(
                by=['Transcript_ID', 'Coding_prob'], ascending=[True, False], na_position='last'
            )
            # Group by Transcript ID and take the first row (which has the highest probability, or is an arbitrary top ORF if probs are NaN)
            best_orfs_found_df = combined_df_sorted.groupby('Transcript_ID').first().reset_index()

    # End of temporary directory block (files inside temp_dir are deleted)

    # Merge aggregated ORF data with all input transcripts
    # This ensures all transcripts from the input BED are present in the final output.
    # Transcript_Len from all_transcripts_with_len_df is the primary one.
    # best_orfs_found_df also contains a Transcript_Len column; drop it to avoid conflict during merge.
    final_df_merged = pd.merge(
        all_transcripts_with_len_df,
        best_orfs_found_df.drop(columns=['Transcript_Len'], errors='ignore'), # errors='ignore' in case Transcript_Len isn't in best_orfs_found_df
        on='Transcript_ID',
        how='left'
    )

    # Ensure all final columns are present and in the correct order, filling with NaN if necessary
    for col_name in final_columns:
        if col_name not in final_df_merged.columns:
            final_df_merged[col_name] = np.nan
    
    final_df_output = final_df_merged[final_columns]
    final_df_output.columns = [col.lower() for col in final_df_output.columns]  # Ensure all columns are lowercase


    # 10. Save aggregated results to Parquet
    logging.info(f"Saving final aggregated results per transcript to {output_parquet_path}")
    try:
        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_parquet_path), exist_ok=True)
        final_df_output.to_parquet(output_parquet_path, index=False)
    except Exception as e:
        logging.error(f"Failed to save results to Parquet file: {e}")
        raise

    logging.info("CPAT calculation and aggregation finished successfully.")
    logging.info(f"Output saved to: {output_parquet_path}")

    # 11. Return the aggregated DataFrame
    return final_df_output


# --- Command Line Interface ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="CPAT: Calculate Coding Potential Assessment Tool scores for a BED file",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Required arguments
    parser.add_argument('-i', '--input_bed', required=True, 
                      help='Input BED file path with transcript coordinates')
    parser.add_argument('-r', '--reference_fasta', required=True, 
                      help='Reference genome FASTA file path')
    parser.add_argument('-o', '--output_parquet', required=True, 
                      help='Output path for the parquet file with CPAT results')
    parser.add_argument('-x', '--hexamer_table', required=True, 
                      help='Hexamer frequency table file (e.g., fly_Hexamer.tsv)')
    parser.add_argument('-m', '--logit_model', required=True, 
                      help='Logistic regression model RData file (e.g., Fly_logitModel.RData)')
    
    # Optional arguments
    parser.add_argument('--min_orf_length', type=int, default=10, 
                      help='Minimum ORF length in nucleotides')
    parser.add_argument('--num_top_orfs', type=int, default=5, 
                      help='Consider top N ORFs per transcript (per strand) before selecting the best overall')
    parser.add_argument('--start_codons', nargs='+', default=["ATG"], 
                      help='List of start codons')
    parser.add_argument('--stop_codons', nargs='+', default=["TAG", "TAA", "TGA"], 
                      help='List of stop codons')
    parser.add_argument('--antisense', action='store_true', 
                      help='Search ORFs on the antisense strand as well')
    parser.add_argument('--verbose', action='store_true', 
                      help='Enable more detailed logging')
    
    # Parse arguments
    args = parser.parse_args()
    
    # Configure logging based on verbosity
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        
    # Extract parameters from arguments
    input_bed = args.input_bed
    reference_fasta = args.reference_fasta
    hexamer_table = args.hexamer_table
    logit_model = args.logit_model
    output_parquet = args.output_parquet
    min_orf_length = args.min_orf_length
    num_top_orfs = args.num_top_orfs
    start_codons = args.start_codons
    stop_codons = args.stop_codons
    antisense = args.antisense

    # Ensure output directory exists
    os.makedirs(os.path.dirname(output_parquet), exist_ok=True)

    # Ensure cpmodule is runnable and Rscript exists before calling the main function
    if "coding_prediction" not in dir(sys.modules.get("cpmodule.utils")):
        logging.error("Error: cpmodule.utils.coding_prediction not found. Cannot proceed.")
        sys.exit(1)
    # Rscript check is done inside run_cpat_calculation

    try:
        # Run the calculation and aggregation
        cpat_results_df = run_cpat_calculation(
            bed_file_path=input_bed,
            ref_genome_path=reference_fasta,
            hexamer_dat_path=hexamer_table,
            logit_model_path=logit_model,
            output_parquet_path=output_parquet,
            min_orf_len=min_orf_length,
            n_top_orf=num_top_orfs,
            start_codons=start_codons,
            stop_codons=stop_codons,
            antisense=antisense
        )

        # Display some results
        logging.info("--- CPAT Aggregated Results Summary ---")
        logging.info(f"Number of transcripts processed: {cpat_results_df.shape[0]}")
        if not cpat_results_df.empty:
            logging.info(f"Mean Coding Probability: {cpat_results_df['coding_prob'].mean():.4f}")
            logging.info(f"Median Coding Probability: {cpat_results_df['coding_prob'].median():.4f}")
            logging.info("First 5 rows of the result:")
            # Still use print for DataFrame display since logging doesn't handle it well
            print(cpat_results_df.head().to_string())
        else:
            logging.info("Result DataFrame is empty.")

        # Verify Parquet file creation
        if os.path.exists(output_parquet) and os.path.getsize(output_parquet) > 0:
            logging.info(f"Results successfully saved to: {output_parquet}")
        elif os.path.exists(output_parquet):
            logging.warning(f"Output Parquet file was created but is empty: {output_parquet}")
        else:
            logging.error(f"Output Parquet file not found at {output_parquet}")

    except FileNotFoundError as e:
        logging.error(f"Execution Error: A required file was not found: {e}")
    except ImportError as e:
        logging.error(f"Execution Error: Failed to import cpmodule components: {e}")
    except EnvironmentError as e:
        logging.error(f"Execution Error: Environment setup issue: {e}")
    except Exception as e:
        logging.error(f"An unexpected error occurred during execution: {e}")
        import traceback
        logging.error("--- Traceback ---")
        traceback.print_exc()
        logging.error("-----------------")