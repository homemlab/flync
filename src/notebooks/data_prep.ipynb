{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9bc80f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06f284e4",
   "metadata": {},
   "source": [
    "## Features\n",
    "- BigWig & BigBed Statistics computed by the `bwq.py` script\n",
    "- Binary K-mer occurences computed by the `kmer.py` script\n",
    "- CPATv3 scores computed by the `cpat.py` script\n",
    "- ViennaRNA package RNA secondary structure Minimum Free Energy values computed by the `mfe.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc8dc2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "# Avoid duplicate handlers in Jupyter\n",
    "if not logging.getLogger().hasHandlers():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from scipy.sparse import load_npz, issparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4ffd0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_redux(sparse_matrix, row_names, col_names, n_components=2, verbose=True):\n",
    "    \"\"\"\n",
    "    Performs dimensionality reduction using the TruncatedSVD method.\n",
    "    Args:\n",
    "        sparse_matrix (scipy.sparse.csr_matrix): The sparse matrix to reduce.\n",
    "        row_names (list): List of row names/IDs.\n",
    "        col_names (list): List of column names/k-mers.\n",
    "        n_components (int): Number of components for dimensionality reduction.\n",
    "        verbose (bool): If True, log messages.\n",
    "    Returns:\n",
    "        tuple: (reduced_matrix, row_names, col_names)\n",
    "               Returns (None, None, None) if an error occurs.\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        logger.info(f\"Performing dimensionality reduction with {n_components} components.\")\n",
    "\n",
    "    try:\n",
    "        svd = TruncatedSVD(n_components=n_components)\n",
    "        reduced_matrix = svd.fit_transform(sparse_matrix)\n",
    "        if verbose:\n",
    "            logger.info(f\"Reduced matrix shape: {reduced_matrix.shape}\")\n",
    "        return reduced_matrix, row_names, col_names\n",
    "    except Exception as e:\n",
    "        if verbose: logger.error(f\"Error during dimensionality reduction: {e}\")\n",
    "        return None, None, None\n",
    "    \n",
    "\n",
    "def dim_redux_by_kmer_length(sparse_matrix, col_names, n_components_per_k=1, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform dimensionality reduction separately for each k-mer length.\n",
    "    Args:\n",
    "        sparse_matrix: scipy.sparse matrix (samples x k-mers)\n",
    "        col_names: list of column names (k-mers)\n",
    "        n_components_per_k: int or dict, number of components per k-mer length\n",
    "        verbose: bool, print progress\n",
    "    Returns:\n",
    "        reduced_matrix: concatenated reduced matrix\n",
    "        reduced_col_names: new column names (with k-mer length info)\n",
    "    \"\"\"\n",
    "    kmer_groups = defaultdict(list)\n",
    "    for idx, name in enumerate(col_names):\n",
    "        kmer_groups[len(name)].append(idx)\n",
    "\n",
    "    reduced_matrices = []\n",
    "    reduced_col_names = []\n",
    "    for k, idxs in sorted(kmer_groups.items()):\n",
    "        if verbose:\n",
    "            logger.info(f\"Reducing {len(idxs)} columns of {k}-mers\")\n",
    "        submatrix = sparse_matrix[:, idxs]\n",
    "        n_comp = n_components_per_k[k] if isinstance(n_components_per_k, dict) and k in n_components_per_k else n_components_per_k\n",
    "        if submatrix.shape[1] <= 1:\n",
    "            # Edge case: cannot reduce further\n",
    "            reduced = submatrix.toarray() if hasattr(submatrix, 'toarray') else submatrix\n",
    "            reduced_matrices.append(reduced)\n",
    "            reduced_col_names.extend([f\"{k}mer_SVD1\"])  # single passthrough\n",
    "            continue\n",
    "        svd = TruncatedSVD(n_components=min(n_comp, submatrix.shape[1]-1))\n",
    "        reduced = svd.fit_transform(submatrix)\n",
    "        reduced_matrices.append(reduced)\n",
    "        reduced_col_names.extend([f\"{k}mer_SVD{i+1}\" for i in range(reduced.shape[1])])\n",
    "    reduced_matrix = np.hstack(reduced_matrices)\n",
    "    return reduced_matrix, reduced_col_names\n",
    "    \n",
    "\n",
    "def load_kmer_results(base_path, redux_n_components, redux=True, group_redux_kmer_len=True, tfidf=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Loads k-mer result files (sparse matrix, row names, column names) from disk.\n",
    "    Correct transformation order: (1) optional TF-IDF weighting on raw counts, then (2) optional SVD (grouped or global).\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Base path to the k-mer result files (without suffix).\n",
    "        redux_n_components (int or dict): Components for dimensionality reduction. If dict, keys are k-mer lengths.\n",
    "        redux (bool): Whether to perform dimensionality reduction.\n",
    "        group_redux_kmer_len (bool): If True, perform SVD separately per k-mer length.\n",
    "        tfidf (bool): Apply TF-IDF weighting BEFORE SVD.\n",
    "        verbose (bool): Log progress if True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (matrix, row_names, col_names) where matrix is TF-IDF weighted and/or SVD reduced.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    sparse_matrix_file = f\"{base_path}_sparse.npz\"\n",
    "    rows_file = f\"{base_path}_rows.txt\"\n",
    "    cols_file = f\"{base_path}_cols.txt\"\n",
    "\n",
    "    if not (os.path.exists(sparse_matrix_file) and os.path.exists(rows_file) and os.path.exists(cols_file)):\n",
    "        binary_sparse_matrix_file = f\"{base_path}_binary_sparse.npz\"\n",
    "        binary_rows_file = f\"{base_path}_binary_rows.txt\"\n",
    "        binary_cols_file = f\"{base_path}_binary_cols.txt\"\n",
    "        if os.path.exists(binary_sparse_matrix_file) and os.path.exists(binary_rows_file) and os.path.exists(binary_cols_file):\n",
    "            sparse_matrix_file = binary_sparse_matrix_file\n",
    "            rows_file = binary_rows_file\n",
    "            cols_file = binary_cols_file\n",
    "            if verbose: logger.info(f\"Loading binary k-mer results from: {base_path}_binary*\")\n",
    "        else:\n",
    "            if verbose: logger.error(f\"One or more result files not found for base path '{base_path}' (tried with and without '_binary' suffix).\")\n",
    "            return None, None, None\n",
    "\n",
    "    if verbose and sparse_matrix_file.startswith(base_path + \"_binary\"):\n",
    "        pass\n",
    "    elif verbose:\n",
    "        logger.info(f\"Loading k-mer results from: {base_path}*\")\n",
    "\n",
    "    try:\n",
    "        sparse_matrix = load_npz(sparse_matrix_file)\n",
    "        with open(rows_file, 'r') as f:\n",
    "            row_names = [line.strip() for line in f]\n",
    "        with open(cols_file, 'r') as f:\n",
    "            col_names = [line.strip() for line in f]\n",
    "        if verbose:\n",
    "            logger.info(f\"Loaded sparse matrix ({sparse_matrix.shape}), {len(row_names)} row names, {len(col_names)} column names.\")\n",
    "    except FileNotFoundError as e:\n",
    "        if verbose: logger.error(f\"File not found: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    # 1. TF-IDF weighting first (if requested)\n",
    "    if tfidf:\n",
    "        if verbose: logger.info(\"Applying TF-IDF transformation BEFORE SVD.\")\n",
    "        transformer = TfidfTransformer()\n",
    "        sparse_matrix = transformer.fit_transform(sparse_matrix)\n",
    "        if verbose: logger.info(f\"TF-IDF weighted matrix shape: {sparse_matrix.shape}\")\n",
    "\n",
    "    # 2. Dimensionality reduction\n",
    "    if redux:\n",
    "        try:\n",
    "            if group_redux_kmer_len:\n",
    "                reduced_matrix, new_cols = dim_redux_by_kmer_length(sparse_matrix, col_names, n_components_per_k=redux_n_components, verbose=verbose)\n",
    "                sparse_matrix = reduced_matrix\n",
    "                col_names = new_cols\n",
    "                if verbose: logger.info(f\"Grouped SVD completed. Shape: {sparse_matrix.shape}\")\n",
    "            else:\n",
    "                reduced_matrix, _, _ = dim_redux(sparse_matrix, row_names, col_names, n_components=redux_n_components, verbose=verbose)\n",
    "                sparse_matrix = reduced_matrix\n",
    "                # col_names replaced with generic component names\n",
    "                col_names = [f\"SVD{i+1}\" for i in range(sparse_matrix.shape[1])]\n",
    "                if verbose: logger.info(f\"Global SVD completed. Shape: {sparse_matrix.shape}\")\n",
    "        except Exception as e:\n",
    "            if verbose: logger.error(f\"Dimensionality reduction failed: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    return sparse_matrix, row_names, col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2c80acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_training_data(\n",
    "    target,\n",
    "    base_parquet_path,\n",
    "    bwq_parquet_path,\n",
    "    cpat_parquet_path,\n",
    "    mfe_parquet_path,\n",
    "    kmer_base_path,\n",
    "    sparse=True,\n",
    "    use_dim_redux=True,\n",
    "    redux_n_components=1,\n",
    "    use_tfidf=True\n",
    "):\n",
    "    base = pd.read_parquet(base_parquet_path)\n",
    "    df = base[['transcript_id', 'Chromosome', 'Start', 'End', 'length', 'gene_name', 'Sequence']]\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    del base\n",
    "\n",
    "    bwq = pd.read_parquet(bwq_parquet_path)\n",
    "    cols_to_drop = ['chromosome', 'start', 'end']\n",
    "    bwq = bwq.drop(columns=[c for c in cols_to_drop if c in bwq.columns])\n",
    "    df = df.merge(bwq, how='inner', on='transcript_id')\n",
    "    del bwq\n",
    "\n",
    "    cpat = pd.read_parquet(cpat_parquet_path)\n",
    "    cols_to_keep = ['transcript_id', 'coding_prob', 'fickett_score', 'hexamer_score', 'orf_len']\n",
    "    cpat = cpat[[c for c in cols_to_keep if c in cpat.columns]]\n",
    "    cpat.rename(columns={\n",
    "        'coding_prob': 'cpat_cod_prob',\n",
    "        'fickett_score': 'cpat_fickett_score',\n",
    "        'hexamer_score': 'cpat_hexamer_score',\n",
    "        'orf_len': 'cpat_orf_len'\n",
    "    }, inplace=True)\n",
    "    df = df.merge(cpat, how='inner', on='transcript_id')\n",
    "    del cpat\n",
    "\n",
    "    mfe = pd.read_parquet(mfe_parquet_path)\n",
    "    mfe_cols_to_keep = ['transcript_id', 'mfe', 'structure']\n",
    "    mfe = mfe[[c for c in mfe_cols_to_keep if c in mfe.columns]]\n",
    "    mfe.rename(columns={'mfe': 'ss_mfe', 'structure': 'ss_structure'}, inplace=True)\n",
    "    df = df.merge(mfe, how='inner', on='transcript_id')\n",
    "    del mfe\n",
    "\n",
    "    npz_mtx, row_names, col_names = load_kmer_results(\n",
    "        kmer_base_path,\n",
    "        redux_n_components=redux_n_components,\n",
    "        redux=use_dim_redux,\n",
    "        group_redux_kmer_len=True,\n",
    "        tfidf=use_tfidf,\n",
    "        verbose=True\n",
    "    )\n",
    "    if npz_mtx is None or row_names is None or col_names is None:\n",
    "        raise FileNotFoundError(f\"Failed to load k-mer results from {kmer_base_path}. Ensure the files exist and are accessible.\")\n",
    "    if len(row_names) != len(set(row_names)):\n",
    "        dupes = len(row_names) - len(set(row_names))\n",
    "        raise ValueError(f\"Duplicate row names found: {dupes} duplicates\")\n",
    "\n",
    "    # Build k-mer DataFrame based on matrix type\n",
    "    if issparse(npz_mtx):\n",
    "        kmer_df = pd.DataFrame.sparse.from_spmatrix(npz_mtx, index=row_names, columns=col_names)\n",
    "        if not sparse:\n",
    "            kmer_df = kmer_df.sparse.to_dense()\n",
    "    else:\n",
    "        # numpy ndarray (SVD output). If sparse requested, this is inconsistent; proceed dense.\n",
    "        if sparse:\n",
    "            logger.warning(\"SVD produced a dense ndarray; overriding sparse=True to use dense DataFrame.\")\n",
    "        kmer_df = pd.DataFrame(npz_mtx, index=row_names, columns=col_names)\n",
    "\n",
    "    df.set_index('transcript_id', inplace=True)\n",
    "    df = df.merge(kmer_df, how='inner', left_index=True, right_index=True)\n",
    "    del kmer_df\n",
    "\n",
    "    if target == 'ncr':\n",
    "        df['y'] = True\n",
    "    elif target == 'pcg':\n",
    "        df['y'] = False\n",
    "    else:\n",
    "        raise ValueError(\"Invalid target: {target}. Must be 'ncr' or 'pcg'.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23904f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 01:33:27,839 - INFO - Loading binary k-mer results from: /home/chlab/flync/new-tests/kmer-feature/ncr_binary_sparse/ncr_binary*\n",
      "2025-09-17 01:33:28,076 - INFO - Loaded sparse matrix ((2981, 8184)), 2981 row names, 8184 column names.\n",
      "2025-09-17 01:33:28,077 - INFO - Applying TF-IDF transformation BEFORE SVD.\n",
      "2025-09-17 01:33:28,076 - INFO - Loaded sparse matrix ((2981, 8184)), 2981 row names, 8184 column names.\n",
      "2025-09-17 01:33:28,077 - INFO - Applying TF-IDF transformation BEFORE SVD.\n",
      "2025-09-17 01:33:28,423 - INFO - TF-IDF weighted matrix shape: (2981, 8184)\n",
      "2025-09-17 01:33:28,424 - INFO - Reducing 8 columns of 3-mers\n",
      "2025-09-17 01:33:28,423 - INFO - TF-IDF weighted matrix shape: (2981, 8184)\n",
      "2025-09-17 01:33:28,424 - INFO - Reducing 8 columns of 3-mers\n",
      "2025-09-17 01:33:28,436 - INFO - Reducing 16 columns of 4-mers\n",
      "2025-09-17 01:33:28,436 - INFO - Reducing 16 columns of 4-mers\n",
      "2025-09-17 01:33:28,451 - INFO - Reducing 32 columns of 5-mers\n",
      "2025-09-17 01:33:28,451 - INFO - Reducing 32 columns of 5-mers\n",
      "2025-09-17 01:33:28,478 - INFO - Reducing 64 columns of 6-mers\n",
      "2025-09-17 01:33:28,478 - INFO - Reducing 64 columns of 6-mers\n",
      "2025-09-17 01:33:28,503 - INFO - Reducing 128 columns of 7-mers\n",
      "2025-09-17 01:33:28,503 - INFO - Reducing 128 columns of 7-mers\n",
      "2025-09-17 01:33:28,539 - INFO - Reducing 256 columns of 8-mers\n",
      "2025-09-17 01:33:28,539 - INFO - Reducing 256 columns of 8-mers\n",
      "2025-09-17 01:33:28,590 - INFO - Reducing 512 columns of 9-mers\n",
      "2025-09-17 01:33:28,590 - INFO - Reducing 512 columns of 9-mers\n",
      "2025-09-17 01:33:28,665 - INFO - Reducing 1024 columns of 10-mers\n",
      "2025-09-17 01:33:28,665 - INFO - Reducing 1024 columns of 10-mers\n",
      "2025-09-17 01:33:28,820 - INFO - Reducing 2048 columns of 11-mers\n",
      "2025-09-17 01:33:28,820 - INFO - Reducing 2048 columns of 11-mers\n",
      "2025-09-17 01:33:28,990 - INFO - Reducing 4096 columns of 12-mers\n",
      "2025-09-17 01:33:28,990 - INFO - Reducing 4096 columns of 12-mers\n",
      "2025-09-17 01:33:29,199 - INFO - Grouped SVD completed. Shape: (2981, 10)\n",
      "2025-09-17 01:33:29,199 - INFO - Grouped SVD completed. Shape: (2981, 10)\n"
     ]
    }
   ],
   "source": [
    "ncr_redux = prep_training_data(\n",
    "    target='ncr',\n",
    "    base_parquet_path='/home/chlab/flync/new-tests/ncr_base.parquet',\n",
    "    bwq_parquet_path='/home/chlab/flync/new-tests/bw-feature/ncr_bwq.parquet',\n",
    "    cpat_parquet_path='/home/chlab/flync/new-tests/cpat-feature/ncr_cpat.parquet',\n",
    "    mfe_parquet_path='/home/chlab/flync/new-tests/mfe-feature/ncr_mfe_linear.parquet',\n",
    "    kmer_base_path='/home/chlab/flync/new-tests/kmer-feature/ncr_binary_sparse/ncr',\n",
    "    redux_n_components=1,\n",
    "    use_dim_redux=True,\n",
    "    use_tfidf=True,\n",
    "    sparse=False\n",
    ")\n",
    "\n",
    "ncr_redux.to_parquet('/home/chlab/flync/src/data/ncr_training_redux.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f121e767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 01:33:29,826 - INFO - Loading binary k-mer results from: /home/chlab/flync/new-tests/kmer-feature/pcg_binary_sparse/pcg_binary*\n",
      "2025-09-17 01:33:33,460 - INFO - Loaded sparse matrix ((30662, 8184)), 30662 row names, 8184 column names.\n",
      "2025-09-17 01:33:33,460 - INFO - Applying TF-IDF transformation BEFORE SVD.\n",
      "2025-09-17 01:33:33,460 - INFO - Loaded sparse matrix ((30662, 8184)), 30662 row names, 8184 column names.\n",
      "2025-09-17 01:33:33,460 - INFO - Applying TF-IDF transformation BEFORE SVD.\n",
      "2025-09-17 01:33:39,653 - INFO - TF-IDF weighted matrix shape: (30662, 8184)\n",
      "2025-09-17 01:33:39,654 - INFO - Reducing 8 columns of 3-mers\n",
      "2025-09-17 01:33:39,653 - INFO - TF-IDF weighted matrix shape: (30662, 8184)\n",
      "2025-09-17 01:33:39,654 - INFO - Reducing 8 columns of 3-mers\n",
      "2025-09-17 01:33:39,817 - INFO - Reducing 16 columns of 4-mers\n",
      "2025-09-17 01:33:39,817 - INFO - Reducing 16 columns of 4-mers\n",
      "2025-09-17 01:33:40,040 - INFO - Reducing 32 columns of 5-mers\n",
      "2025-09-17 01:33:40,040 - INFO - Reducing 32 columns of 5-mers\n",
      "2025-09-17 01:33:40,271 - INFO - Reducing 64 columns of 6-mers\n",
      "2025-09-17 01:33:40,271 - INFO - Reducing 64 columns of 6-mers\n",
      "2025-09-17 01:33:40,571 - INFO - Reducing 128 columns of 7-mers\n",
      "2025-09-17 01:33:40,571 - INFO - Reducing 128 columns of 7-mers\n",
      "2025-09-17 01:33:40,973 - INFO - Reducing 256 columns of 8-mers\n",
      "2025-09-17 01:33:40,973 - INFO - Reducing 256 columns of 8-mers\n",
      "2025-09-17 01:33:41,501 - INFO - Reducing 512 columns of 9-mers\n",
      "2025-09-17 01:33:41,501 - INFO - Reducing 512 columns of 9-mers\n",
      "2025-09-17 01:33:42,269 - INFO - Reducing 1024 columns of 10-mers\n",
      "2025-09-17 01:33:42,269 - INFO - Reducing 1024 columns of 10-mers\n",
      "2025-09-17 01:33:43,537 - INFO - Reducing 2048 columns of 11-mers\n",
      "2025-09-17 01:33:43,537 - INFO - Reducing 2048 columns of 11-mers\n",
      "2025-09-17 01:33:45,322 - INFO - Reducing 4096 columns of 12-mers\n",
      "2025-09-17 01:33:45,322 - INFO - Reducing 4096 columns of 12-mers\n",
      "2025-09-17 01:33:47,753 - INFO - Grouped SVD completed. Shape: (30662, 10)\n",
      "2025-09-17 01:33:47,753 - INFO - Grouped SVD completed. Shape: (30662, 10)\n"
     ]
    }
   ],
   "source": [
    "pcg_redux = prep_training_data(\n",
    "    target='pcg',\n",
    "    base_parquet_path='/home/chlab/flync/new-tests/pcg_base.parquet',\n",
    "    bwq_parquet_path='/home/chlab/flync/new-tests/bw-feature/pcg_bwq.parquet',\n",
    "    cpat_parquet_path='/home/chlab/flync/new-tests/cpat-feature/pcg_cpat.parquet',\n",
    "    mfe_parquet_path='/home/chlab/flync/new-tests/mfe-feature/pcg_mfe_linear.parquet',\n",
    "    kmer_base_path='/home/chlab/flync/new-tests/kmer-feature/pcg_binary_sparse/pcg',\n",
    "    redux_n_components=1,\n",
    "    use_dim_redux=True,\n",
    "    use_tfidf=True,\n",
    "    sparse=False\n",
    ")\n",
    "\n",
    "pcg_redux.to_parquet('/home/chlab/flync/src/data/pcg_training_redux.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac730d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncr_full = prep_training_data(\n",
    "#     target='ncr',\n",
    "#     base_parquet_path='/home/chlab/flync/new-tests/ncr_base.parquet',\n",
    "#     bwq_parquet_path='/home/chlab/flync/new-tests/bw-feature/ncr_bwq.parquet',\n",
    "#     cpat_parquet_path='/home/chlab/flync/new-tests/cpat-feature/ncr_cpat.parquet',\n",
    "#     mfe_parquet_path='/home/chlab/flync/new-tests/mfe-feature/ncr_mfe_linear.parquet',\n",
    "#     kmer_base_path='/home/chlab/flync/new-tests/kmer-feature/ncr_binary_sparse/ncr',\n",
    "#     redux_n_components=None,\n",
    "#     use_dim_redux=False,\n",
    "#     use_tfidf=True,\n",
    "#     sparse=False\n",
    "# )\n",
    "\n",
    "# ncr_full.to_parquet('/home/chlab/flync/src/data/ncr_training_full.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14a241ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcg_full = prep_training_data(\n",
    "#     target='pcg',\n",
    "#     base_parquet_path='/home/chlab/flync/new-tests/pcg_base.parquet',\n",
    "#     bwq_parquet_path='/home/chlab/flync/new-tests/bw-feature/pcg_bwq.parquet',\n",
    "#     cpat_parquet_path='/home/chlab/flync/new-tests/cpat-feature/pcg_cpat.parquet',\n",
    "#     mfe_parquet_path='/home/chlab/flync/new-tests/mfe-feature/pcg_mfe_linear.parquet',\n",
    "#     kmer_base_path='/home/chlab/flync/new-tests/kmer-feature/pcg_binary_sparse/pcg',\n",
    "#     redux_n_components=None,\n",
    "#     use_dim_redux=False,\n",
    "#     use_tfidf=True,\n",
    "#     sparse=False\n",
    "# )\n",
    "\n",
    "# pcg_full.to_parquet('/home/chlab/flync/src/data/pcg_training_full.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flync",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
