{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9bc80f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06f284e4",
   "metadata": {},
   "source": [
    "## Features\n",
    "- BigWig & BigBed Statistics computed by the `bwq.py` script\n",
    "- Binary K-mer occurences computed by the `kmer.py` script\n",
    "- CPATv3 scores computed by the `cpat.py` script\n",
    "- ViennaRNA package RNA secondary structure Minimum Free Energy values computed by the `mfe.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc8dc2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "# Avoid duplicate handlers in Jupyter\n",
    "if not logging.getLogger().hasHandlers():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from scipy.sparse import load_npz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c40c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_redux(sparse_matrix, row_names, col_names, n_components=2, verbose=True):\n",
    "    \"\"\"\n",
    "    Performs dimensionality reduction using the TruncatedSVD method.\n",
    "    Args:\n",
    "        sparse_matrix (scipy.sparse.csr_matrix): The sparse matrix to reduce.\n",
    "        row_names (list): List of row names/IDs.\n",
    "        col_names (list): List of column names/k-mers.\n",
    "        n_components (int): Number of components for dimensionality reduction.\n",
    "        verbose (bool): If True, log messages.\n",
    "    Returns:\n",
    "        tuple: (reduced_matrix, row_names, col_names)\n",
    "               Returns (None, None, None) if an error occurs.\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        logger.info(f\"Performing dimensionality reduction with {n_components} components.\")\n",
    "\n",
    "    try:\n",
    "        svd = TruncatedSVD(n_components=n_components)\n",
    "        reduced_matrix = svd.fit_transform(sparse_matrix)\n",
    "        if verbose:\n",
    "            logger.info(f\"Reduced matrix shape: {reduced_matrix.shape}\")\n",
    "        return reduced_matrix, row_names, col_names\n",
    "    except Exception as e:\n",
    "        if verbose: logger.error(f\"Error during dimensionality reduction: {e}\")\n",
    "        return None, None, None\n",
    "    \n",
    "def dim_redux_by_kmer_length(sparse_matrix, col_names, n_components_per_k=1, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform dimensionality reduction separately for each k-mer length.\n",
    "    Args:\n",
    "        sparse_matrix: scipy.sparse matrix (samples x k-mers)\n",
    "        col_names: list of column names (k-mers)\n",
    "        n_components_per_k: int or dict, number of components per k-mer length\n",
    "        verbose: bool, print progress\n",
    "    Returns:\n",
    "        reduced_matrix: concatenated reduced matrix\n",
    "        reduced_col_names: new column names (with k-mer length info)\n",
    "    \"\"\"\n",
    "    # Group columns by k-mer length\n",
    "    kmer_groups = defaultdict(list)\n",
    "    for idx, name in enumerate(col_names):\n",
    "        kmer_groups[len(name)].append(idx)\n",
    "\n",
    "    reduced_matrices = []\n",
    "    reduced_col_names = []\n",
    "    for k, idxs in sorted(kmer_groups.items()):\n",
    "        if verbose:\n",
    "            logger.info(f\"Reducing {len(idxs)} columns of {k}-mers\")\n",
    "        submatrix = sparse_matrix[:, idxs]\n",
    "        n_comp = n_components_per_k[k] if isinstance(n_components_per_k, dict) and k in n_components_per_k else n_components_per_k\n",
    "        svd = TruncatedSVD(n_components=min(n_comp, submatrix.shape[1]-1))\n",
    "        reduced = svd.fit_transform(submatrix)\n",
    "        reduced_matrices.append(reduced)\n",
    "        reduced_col_names.extend([f\"{k}mer_SVD{i+1}\" for i in range(reduced.shape[1])])\n",
    "    # Concatenate all reduced matrices horizontally\n",
    "    reduced_matrix = np.hstack(reduced_matrices)\n",
    "    return reduced_matrix, reduced_col_names\n",
    "    \n",
    "def load_kmer_results(base_path, redux_n_components, redux=True, group_redux_kmer_len=True, tfidf=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Loads k-mer result files (sparse matrix, row names, column names) from disk, with optional dimensionality reduction and TF-IDF transformation.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Base path to the k-mer result files (without suffix).\n",
    "        redux_n_components (int or dict): Number of components for dimensionality reduction. If a dict, keys are k-mer lengths.\n",
    "        redux (bool, optional): Whether to perform dimensionality reduction. Defaults to True.\n",
    "        group_redux_kmer_len (bool, optional): If True, performs dimensionality reduction separately for each k-mer length. Defaults to True.\n",
    "        tfidf (bool, optional): Whether to apply TF-IDF transformation to the matrix. Defaults to True.\n",
    "        verbose (bool, optional): If True, logs progress and info messages. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (sparse_matrix, row_names, col_names)\n",
    "            - sparse_matrix: The loaded (and optionally reduced/transformed) matrix (scipy.sparse or numpy.ndarray).\n",
    "            - row_names: List of row/sample names.\n",
    "            - col_names: List of column/k-mer names (may be reduced).\n",
    "        Returns (None, None, None) if loading or processing fails.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    # Determine file paths based on whether it's likely binary or ATGC output\n",
    "    # This is a simple heuristic; a more robust way would be to pass the exact filenames\n",
    "    # or have a metadata file.\n",
    "    sparse_matrix_file = f\"{base_path}_sparse.npz\"\n",
    "    rows_file = f\"{base_path}_rows.txt\"\n",
    "    cols_file = f\"{base_path}_cols.txt\"\n",
    "\n",
    "    if not (os.path.exists(sparse_matrix_file) and os.path.exists(rows_file) and os.path.exists(cols_file)):\n",
    "         # Try with _binary suffix if primary files not found\n",
    "         binary_sparse_matrix_file = f\"{base_path}_binary_sparse.npz\"\n",
    "         binary_rows_file = f\"{base_path}_binary_rows.txt\"\n",
    "         binary_cols_file = f\"{base_path}_binary_cols.txt\"\n",
    "         if os.path.exists(binary_sparse_matrix_file) and os.path.exists(binary_rows_file) and os.path.exists(binary_cols_file):\n",
    "             sparse_matrix_file = binary_sparse_matrix_file\n",
    "             rows_file = binary_rows_file\n",
    "             cols_file = binary_cols_file\n",
    "             if verbose: logger.info(f\"Loading binary k-mer results from: {base_path}_binary*\")\n",
    "         else:\n",
    "            if verbose: logger.error(f\"One or more result files not found for base path '{base_path}' (tried with and without '_binary' suffix).\")\n",
    "            return None, None, None\n",
    "\n",
    "    if verbose and sparse_matrix_file.startswith(base_path + \"_binary\"):\n",
    "        pass # Already logged above\n",
    "    elif verbose:\n",
    "        logger.info(f\"Loading k-mer results from: {base_path}*\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        sparse_matrix = load_npz(sparse_matrix_file)\n",
    "        with open(rows_file, 'r') as f:\n",
    "            row_names = [line.strip() for line in f]\n",
    "        with open(cols_file, 'r') as f:\n",
    "            col_names = [line.strip() for line in f]\n",
    "        if verbose:\n",
    "            logger.info(f\"Loaded sparse matrix ({sparse_matrix.shape}), {len(row_names)} row names, {len(col_names)} column names.\")\n",
    "    except FileNotFoundError as e:\n",
    "        if verbose: logger.error(f\"File not found: {e}\")\n",
    "        return None, None, None\n",
    "        \n",
    "    if redux:\n",
    "        try:\n",
    "            if group_redux_kmer_len:\n",
    "                # Perform dimensionality reduction by k-mer length\n",
    "                reduced_matrix, col_names = dim_redux_by_kmer_length(sparse_matrix, col_names, n_components_per_k=redux_n_components, verbose=verbose)\n",
    "                logger.info(f\"Dimensionality reduction by k-mer length completed. Reduced matrix shape: {reduced_matrix.shape}\")\n",
    "                logger.info(f\"Number of rows: {len(row_names)}, Number of columns: {len(col_names)}\")\n",
    "            else:\n",
    "                # Perform dimensionality reduction on the entire matrix\n",
    "                reduced_matrix, row_names, col_names = dim_redux(sparse_matrix, row_names, col_names, n_components=redux_n_components, verbose=verbose)\n",
    "                logger.info(f\"Dimensionality reduction completed. Reduced matrix shape: {reduced_matrix.shape}\")\n",
    "                logger.info(f\"Number of rows: {len(row_names)}, Number of columns: {len(col_names)}\")\n",
    "        except Exception as e:\n",
    "            if verbose: logger.error(f\"Dimensionality reduction failed: {e}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        if reduced_matrix is not None:\n",
    "            sparse_matrix = reduced_matrix\n",
    "        else:\n",
    "            if verbose: logger.error(\"Dimensionality reduction failed.\")\n",
    "            return None, None, None\n",
    "        \n",
    "    if tfidf:\n",
    "        # Apply TF-IDF transformation if requested\n",
    "        if verbose: logger.info(\"Applying TF-IDF transformation.\")\n",
    "        tfidf = TfidfTransformer()\n",
    "        sparse_matrix = tfidf.fit_transform(sparse_matrix)\n",
    "        if verbose: logger.info(f\"TF-IDF transformed matrix shape: {sparse_matrix.shape}\")\n",
    "    \n",
    "    return sparse_matrix, row_names, col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ffd0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_redux(sparse_matrix, row_names, col_names, n_components=2, verbose=True):\n",
    "    \"\"\"\n",
    "    Performs dimensionality reduction using the TruncatedSVD method.\n",
    "    Args:\n",
    "        sparse_matrix (scipy.sparse.csr_matrix): The sparse matrix to reduce.\n",
    "        row_names (list): List of row names/IDs.\n",
    "        col_names (list): List of column names/k-mers.\n",
    "        n_components (int): Number of components for dimensionality reduction.\n",
    "        verbose (bool): If True, log messages.\n",
    "    Returns:\n",
    "        tuple: (reduced_matrix, row_names, col_names)\n",
    "               Returns (None, None, None) if an error occurs.\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        logger.info(f\"Performing dimensionality reduction with {n_components} components.\")\n",
    "\n",
    "    try:\n",
    "        svd = TruncatedSVD(n_components=n_components)\n",
    "        reduced_matrix = svd.fit_transform(sparse_matrix)\n",
    "        if verbose:\n",
    "            logger.info(f\"Reduced matrix shape: {reduced_matrix.shape}\")\n",
    "        return reduced_matrix, row_names, col_names\n",
    "    except Exception as e:\n",
    "        if verbose: logger.error(f\"Error during dimensionality reduction: {e}\")\n",
    "        return None, None, None\n",
    "    \n",
    "def dim_redux_by_kmer_length(sparse_matrix, col_names, n_components_per_k=1, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform dimensionality reduction separately for each k-mer length.\n",
    "    Args:\n",
    "        sparse_matrix: scipy.sparse matrix (samples x k-mers)\n",
    "        col_names: list of column names (k-mers)\n",
    "        n_components_per_k: int or dict, number of components per k-mer length\n",
    "        verbose: bool, print progress\n",
    "    Returns:\n",
    "        reduced_matrix: concatenated reduced matrix\n",
    "        reduced_col_names: new column names (with k-mer length info)\n",
    "    \"\"\"\n",
    "    # Group columns by k-mer length\n",
    "    kmer_groups = defaultdict(list)\n",
    "    for idx, name in enumerate(col_names):\n",
    "        kmer_groups[len(name)].append(idx)\n",
    "\n",
    "    reduced_matrices = []\n",
    "    reduced_col_names = []\n",
    "    for k, idxs in sorted(kmer_groups.items()):\n",
    "        if verbose:\n",
    "            logger.info(f\"Reducing {len(idxs)} columns of {k}-mers\")\n",
    "        submatrix = sparse_matrix[:, idxs]\n",
    "        n_comp = n_components_per_k[k] if isinstance(n_components_per_k, dict) and k in n_components_per_k else n_components_per_k\n",
    "        svd = TruncatedSVD(n_components=min(n_comp, submatrix.shape[1]-1))\n",
    "        reduced = svd.fit_transform(submatrix)\n",
    "        reduced_matrices.append(reduced)\n",
    "        reduced_col_names.extend([f\"{k}mer_SVD{i+1}\" for i in range(reduced.shape[1])])\n",
    "    # Concatenate all reduced matrices horizontally\n",
    "    reduced_matrix = np.hstack(reduced_matrices)\n",
    "    return reduced_matrix, reduced_col_names\n",
    "    \n",
    "def load_kmer_results(base_path, redux_n_components, redux=True, group_redux_kmer_len=True, tfidf=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Loads k-mer result files (sparse matrix, row names, column names) from disk, with optional dimensionality reduction and TF-IDF transformation.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Base path to the k-mer result files (without suffix).\n",
    "        redux_n_components (int or dict): Number of components for dimensionality reduction. If a dict, keys are k-mer lengths.\n",
    "        redux (bool, optional): Whether to perform dimensionality reduction. Defaults to True.\n",
    "        group_redux_kmer_len (bool, optional): If True, performs dimensionality reduction separately for each k-mer length. Defaults to True.\n",
    "        tfidf (bool, optional): Whether to apply TF-IDF transformation to the matrix. Defaults to True.\n",
    "        verbose (bool, optional): If True, logs progress and info messages. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (sparse_matrix, row_names, col_names)\n",
    "            - sparse_matrix: The loaded (and optionally reduced/transformed) matrix (scipy.sparse or numpy.ndarray).\n",
    "            - row_names: List of row/sample names.\n",
    "            - col_names: List of column/k-mer names (may be reduced).\n",
    "        Returns (None, None, None) if loading or processing fails.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    # Determine file paths based on whether it's likely binary or ATGC output\n",
    "    # This is a simple heuristic; a more robust way would be to pass the exact filenames\n",
    "    # or have a metadata file.\n",
    "    sparse_matrix_file = f\"{base_path}_sparse.npz\"\n",
    "    rows_file = f\"{base_path}_rows.txt\"\n",
    "    cols_file = f\"{base_path}_cols.txt\"\n",
    "\n",
    "    if not (os.path.exists(sparse_matrix_file) and os.path.exists(rows_file) and os.path.exists(cols_file)):\n",
    "         # Try with _binary suffix if primary files not found\n",
    "         binary_sparse_matrix_file = f\"{base_path}_binary_sparse.npz\"\n",
    "         binary_rows_file = f\"{base_path}_binary_rows.txt\"\n",
    "         binary_cols_file = f\"{base_path}_binary_cols.txt\"\n",
    "         if os.path.exists(binary_sparse_matrix_file) and os.path.exists(binary_rows_file) and os.path.exists(binary_cols_file):\n",
    "             sparse_matrix_file = binary_sparse_matrix_file\n",
    "             rows_file = binary_rows_file\n",
    "             cols_file = binary_cols_file\n",
    "             if verbose: logger.info(f\"Loading binary k-mer results from: {base_path}_binary*\")\n",
    "         else:\n",
    "            if verbose: logger.error(f\"One or more result files not found for base path '{base_path}' (tried with and without '_binary' suffix).\")\n",
    "            return None, None, None\n",
    "\n",
    "    if verbose and sparse_matrix_file.startswith(base_path + \"_binary\"):\n",
    "        pass # Already logged above\n",
    "    elif verbose:\n",
    "        logger.info(f\"Loading k-mer results from: {base_path}*\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        sparse_matrix = load_npz(sparse_matrix_file)\n",
    "        with open(rows_file, 'r') as f:\n",
    "            row_names = [line.strip() for line in f]\n",
    "        with open(cols_file, 'r') as f:\n",
    "            col_names = [line.strip() for line in f]\n",
    "        if verbose:\n",
    "            logger.info(f\"Loaded sparse matrix ({sparse_matrix.shape}), {len(row_names)} row names, {len(col_names)} column names.\")\n",
    "    except FileNotFoundError as e:\n",
    "        if verbose: logger.error(f\"File not found: {e}\")\n",
    "        return None, None, None\n",
    "        \n",
    "    if redux:\n",
    "        try:\n",
    "            if group_redux_kmer_len:\n",
    "                # Perform dimensionality reduction by k-mer length\n",
    "                reduced_matrix, col_names = dim_redux_by_kmer_length(sparse_matrix, col_names, n_components_per_k=redux_n_components, verbose=verbose)\n",
    "                logger.info(f\"Dimensionality reduction by k-mer length completed. Reduced matrix shape: {reduced_matrix.shape}\")\n",
    "                logger.info(f\"Number of rows: {len(row_names)}, Number of columns: {len(col_names)}\")\n",
    "            else:\n",
    "                # Perform dimensionality reduction on the entire matrix\n",
    "                reduced_matrix, row_names, col_names = dim_redux(sparse_matrix, row_names, col_names, n_components=redux_n_components, verbose=verbose)\n",
    "                logger.info(f\"Dimensionality reduction completed. Reduced matrix shape: {reduced_matrix.shape}\")\n",
    "                logger.info(f\"Number of rows: {len(row_names)}, Number of columns: {len(col_names)}\")\n",
    "        except Exception as e:\n",
    "            if verbose: logger.error(f\"Dimensionality reduction failed: {e}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        if reduced_matrix is not None:\n",
    "            sparse_matrix = reduced_matrix\n",
    "        else:\n",
    "            if verbose: logger.error(\"Dimensionality reduction failed.\")\n",
    "            return None, None, None\n",
    "        \n",
    "    if tfidf:\n",
    "        # Apply TF-IDF transformation if requested\n",
    "        if verbose: logger.info(\"Applying TF-IDF transformation.\")\n",
    "        tfidf = TfidfTransformer()\n",
    "        sparse_matrix = tfidf.fit_transform(sparse_matrix)\n",
    "        if verbose: logger.info(f\"TF-IDF transformed matrix shape: {sparse_matrix.shape}\")\n",
    "    \n",
    "    return sparse_matrix, row_names, col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2c80acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_training_data(\n",
    "    target,\n",
    "    base_parquet_path,\n",
    "    bwq_parquet_path,\n",
    "    cpat_parquet_path,\n",
    "    mfe_parquet_path,\n",
    "    kmer_base_path,\n",
    "    sparse=True,\n",
    "    use_dim_redux=True,\n",
    "    redux_n_components=1,\n",
    "    use_tfidf=True\n",
    "):\n",
    "    \n",
    "    base = pd.read_parquet(base_parquet_path)\n",
    "    # generate a unique ID for each row of each dataframe\n",
    "    base['id'] = base.apply(lambda x: f\"{x['transcript_id']}_{x['exon_number']}_{x['Start']}_{x['End']}\" if pd.notnull(x['exon_number']) else f\"{x['transcript_id']}_{x['Start']}_{x['End']}\", axis=1)\n",
    "    # get relevant columns from base\n",
    "    df = base[['id', 'Chromosome', 'Start', 'End', 'length', 'transcript_id', 'exon_number', 'gene_name', 'Sequence']]\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    # clear base dataframe\n",
    "    del base\n",
    "\n",
    "    # load bwq dataframe and generate a unique ID for each row\n",
    "    bwq = pd.read_parquet(bwq_parquet_path)\n",
    "    bwq['id'] = bwq.apply(lambda x: f\"{x['name']}_{x['start']}_{x['end']}\", axis=1)\n",
    "    # drop unnecessary columns\n",
    "    cols_to_drop = ['chromosome', 'start', 'end', 'name']\n",
    "    bwq = bwq.drop(columns=cols_to_drop)\n",
    "    # inner join df with bwq and assert that the number of rows is the same\n",
    "    df = df.merge(bwq, how='inner', left_on=['id'], right_on=['id'])\n",
    "    del bwq\n",
    "\n",
    "    cpat = pd.read_parquet(cpat_parquet_path)\n",
    "    cols_to_keep = ['Transcript_ID', 'Coding_prob', 'Fickett_Score', 'Hexamer_Score', 'ORF_Len']\n",
    "    cpat = cpat[cols_to_keep]\n",
    "    cpat.rename(columns={\n",
    "        'Transcript_ID': 'id',\n",
    "        'Coding_prob': 'cpat_cod_prob',\n",
    "        'Fickett_Score': 'cpat_fickett_score',\n",
    "        'Hexamer_Score': 'cpat_hexamer_score',\n",
    "        'ORF_Len': 'cpat_orf_len'\n",
    "    }, inplace=True)\n",
    "    # inner join df with cpat and assert that the number of rows is the same\n",
    "    df = df.merge(cpat, how='inner', left_on=['id'], right_on=['id'])\n",
    "    del cpat\n",
    "\n",
    "    mfe = pd.read_parquet(mfe_parquet_path)\n",
    "    mfe['id'] = mfe.apply(lambda x: f\"{x['transcript_id']}_{x['exon_number']}_{x['Start']}_{x['End']}\" if pd.notnull(x['exon_number']) else f\"{x['transcript_id']}_{x['Start']}_{x['End']}\", axis=1)\n",
    "    cols_to_keep = ['id', 'MFE', 'Structure']\n",
    "    mfe = mfe[cols_to_keep]\n",
    "    mfe.rename(columns={\n",
    "        'MFE': 'vrna_mfe',\n",
    "        'Structure': 'vrna_structure'\n",
    "    }, inplace=True)\n",
    "    df = df.merge(mfe, how='inner', left_on=['id'], right_on=['id'])\n",
    "    del mfe\n",
    "\n",
    "    npz_mtx, row_names, col_names = load_kmer_results(kmer_base_path, \n",
    "                                                      redux_n_components=redux_n_components, \n",
    "                                                      redux=use_dim_redux, \n",
    "                                                      group_redux_kmer_len=True, \n",
    "                                                      tfidf=use_tfidf, \n",
    "                                                      verbose=True)\n",
    "    if npz_mtx is None or row_names is None or col_names is None:\n",
    "        raise FileNotFoundError(f\"Failed to load k-mer results from {kmer_base_path}. Ensure the files exist and are accessible.\")\n",
    "    assert len(row_names) == len(set(row_names)), f\"Duplicate row names found: {len(row_names) - len(set(row_names))} duplicates\"\n",
    "\n",
    "    kmer = pd.DataFrame.sparse.from_spmatrix(npz_mtx, columns=col_names, index=row_names)\n",
    "\n",
    "    if not sparse:\n",
    "        try:\n",
    "            # Convert sparse matrix to dense\n",
    "            kmer = kmer.sparse.to_dense()\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error converting sparse matrix to dense: {e}\")\n",
    "\n",
    "    # set index of df as 'id' to merge with kmer sparse matrix\n",
    "    df.set_index('id', inplace=True)\n",
    "    df = df.merge(kmer, how='inner', left_index=True, right_index=True)\n",
    "    del kmer\n",
    "\n",
    "    if target == 'ncr':\n",
    "        df['y'] = True\n",
    "        return df\n",
    "    elif target == 'pcg':\n",
    "        df['y'] = False\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid target: {target}. Must be 'ncr' or 'pcg'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23904f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:56:28,964 - INFO - Loading binary k-mer results from: /home/chlab/flync/new-tests/kmer-feature/ncr_binary_sparse/ncr_binary*\n",
      "2025-05-20 16:56:29,538 - INFO - Loaded sparse matrix ((5455, 8184)), 5455 row names, 8184 column names.\n",
      "2025-05-20 16:56:29,540 - INFO - Reducing 8 columns of 3-mers\n",
      "2025-05-20 16:56:29,598 - INFO - Reducing 16 columns of 4-mers\n",
      "2025-05-20 16:56:29,643 - INFO - Reducing 32 columns of 5-mers\n",
      "2025-05-20 16:56:29,694 - INFO - Reducing 64 columns of 6-mers\n",
      "2025-05-20 16:56:29,784 - INFO - Reducing 128 columns of 7-mers\n",
      "2025-05-20 16:56:29,899 - INFO - Reducing 256 columns of 8-mers\n",
      "2025-05-20 16:56:30,100 - INFO - Reducing 512 columns of 9-mers\n",
      "2025-05-20 16:56:30,343 - INFO - Reducing 1024 columns of 10-mers\n",
      "2025-05-20 16:56:30,771 - INFO - Reducing 2048 columns of 11-mers\n",
      "2025-05-20 16:56:31,577 - INFO - Reducing 4096 columns of 12-mers\n",
      "2025-05-20 16:56:32,705 - INFO - Dimensionality reduction by k-mer length completed. Reduced matrix shape: (5455, 10)\n",
      "2025-05-20 16:56:32,707 - INFO - Number of rows: 5455, Number of columns: 10\n",
      "2025-05-20 16:56:32,708 - INFO - Applying TF-IDF transformation.\n",
      "2025-05-20 16:56:32,713 - INFO - TF-IDF transformed matrix shape: (5455, 10)\n"
     ]
    }
   ],
   "source": [
    "ncr_redux = prep_training_data(\n",
    "    target='ncr',\n",
    "    base_parquet_path='/home/chlab/flync/new-tests/ncr_base.parquet',\n",
    "    bwq_parquet_path='/home/chlab/flync/new-tests/bw-feature/ncr_bwq.parquet',\n",
    "    cpat_parquet_path='/home/chlab/flync/new-tests/cpat-feature/ncr_cpat.parquet',\n",
    "    mfe_parquet_path='/home/chlab/flync/new-tests/mfe-feature/ncr_mfe.parquet',\n",
    "    kmer_base_path='/home/chlab/flync/new-tests/kmer-feature/ncr_binary_sparse/ncr',\n",
    "    redux_n_components=1,\n",
    "    use_dim_redux=True,\n",
    "    use_tfidf=True,\n",
    "    sparse=False\n",
    ")\n",
    "\n",
    "ncr_redux.to_parquet('/home/chlab/flync/src/data/ncr_training_redux.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f121e767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:56:38,787 - INFO - Loading binary k-mer results from: /home/chlab/flync/new-tests/kmer-feature/pcg_binary_sparse/pcg_binary*\n",
      "2025-05-20 16:56:49,326 - INFO - Loaded sparse matrix ((98447, 8184)), 98447 row names, 8184 column names.\n",
      "2025-05-20 16:56:49,327 - INFO - Reducing 8 columns of 3-mers\n",
      "2025-05-20 16:56:50,080 - INFO - Reducing 16 columns of 4-mers\n",
      "2025-05-20 16:56:50,951 - INFO - Reducing 32 columns of 5-mers\n",
      "2025-05-20 16:56:51,867 - INFO - Reducing 64 columns of 6-mers\n",
      "2025-05-20 16:56:53,208 - INFO - Reducing 128 columns of 7-mers\n",
      "2025-05-20 16:56:55,085 - INFO - Reducing 256 columns of 8-mers\n",
      "2025-05-20 16:56:58,007 - INFO - Reducing 512 columns of 9-mers\n",
      "2025-05-20 16:57:01,964 - INFO - Reducing 1024 columns of 10-mers\n",
      "2025-05-20 16:57:07,813 - INFO - Reducing 2048 columns of 11-mers\n",
      "2025-05-20 16:57:16,332 - INFO - Reducing 4096 columns of 12-mers\n",
      "2025-05-20 16:57:28,035 - INFO - Dimensionality reduction by k-mer length completed. Reduced matrix shape: (98447, 10)\n",
      "2025-05-20 16:57:28,037 - INFO - Number of rows: 98447, Number of columns: 10\n",
      "2025-05-20 16:57:28,047 - INFO - Applying TF-IDF transformation.\n",
      "2025-05-20 16:57:28,142 - INFO - TF-IDF transformed matrix shape: (98447, 10)\n"
     ]
    }
   ],
   "source": [
    "pcg_redux = prep_training_data(\n",
    "    target='pcg',\n",
    "    base_parquet_path='/home/chlab/flync/new-tests/pcg_base.parquet',\n",
    "    bwq_parquet_path='/home/chlab/flync/new-tests/bw-feature/pcg_bwq.parquet',\n",
    "    cpat_parquet_path='/home/chlab/flync/new-tests/cpat-feature/pcg_cpat.parquet',\n",
    "    mfe_parquet_path='/home/chlab/flync/new-tests/mfe-feature/pcg_mfe.parquet',\n",
    "    kmer_base_path='/home/chlab/flync/new-tests/kmer-feature/pcg_binary_sparse/pcg',\n",
    "    redux_n_components=1,\n",
    "    use_dim_redux=True,\n",
    "    use_tfidf=True,\n",
    "    sparse=False\n",
    ")\n",
    "\n",
    "pcg_redux.to_parquet('/home/chlab/flync/src/data/pcg_training_redux.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac730d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncr_full = prep_training_data(\n",
    "    target='ncr',\n",
    "    base_parquet_path='/home/chlab/flync/new-tests/ncr_base.parquet',\n",
    "    bwq_parquet_path='/home/chlab/flync/new-tests/bw-feature/ncr_bwq.parquet',\n",
    "    cpat_parquet_path='/home/chlab/flync/new-tests/cpat-feature/ncr_cpat.parquet',\n",
    "    mfe_parquet_path='/home/chlab/flync/new-tests/mfe-feature/ncr_mfe.parquet',\n",
    "    kmer_base_path='/home/chlab/flync/new-tests/kmer-feature/ncr_binary_sparse/ncr',\n",
    "    redux_n_components=None,\n",
    "    use_dim_redux=False,\n",
    "    use_tfidf=True,\n",
    "    sparse=False\n",
    ")\n",
    "\n",
    "ncr_full.to_parquet('/home/chlab/flync/src/data/ncr_training_full.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a241ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcg_full = prep_training_data(\n",
    "    target='pcg',\n",
    "    base_parquet_path='/home/chlab/flync/new-tests/pcg_base.parquet',\n",
    "    bwq_parquet_path='/home/chlab/flync/new-tests/bw-feature/pcg_bwq.parquet',\n",
    "    cpat_parquet_path='/home/chlab/flync/new-tests/cpat-feature/pcg_cpat.parquet',\n",
    "    mfe_parquet_path='/home/chlab/flync/new-tests/mfe-feature/pcg_mfe.parquet',\n",
    "    kmer_base_path='/home/chlab/flync/new-tests/kmer-feature/pcg_binary_sparse/pcg',\n",
    "    redux_n_components=None,\n",
    "    use_dim_redux=False,\n",
    "    use_tfidf=True,\n",
    "    sparse=False\n",
    ")\n",
    "\n",
    "pcg_full.to_parquet('/home/chlab/flync/src/data/pcg_training_full.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c493685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flync",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
